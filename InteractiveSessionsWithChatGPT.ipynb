{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmVYLmaP-ClG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "L0s0R5Kj951N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from ipywidgets import widgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull branch from private gpt demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Gkqn80Y-9-eV",
    "outputId": "82005d9d-3162-47d8-b643-d08f23524fcc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" \n",
       "      href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css\" \n",
       "      integrity=\"sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2\" \n",
       "      crossorigin=\"anonymous\">\n",
       "<style>\n",
       "    body{margin-top:20px;}\n",
       "\n",
       "    .chat-message-left,\n",
       "    .chat-message-right {\n",
       "        display: flex;\n",
       "        flex-shrink: 0\n",
       "    }\n",
       "\n",
       "    .chat-message-left {\n",
       "        margin-right: auto\n",
       "    }\n",
       "\n",
       "    .chat-message-right {\n",
       "        flex-direction: row-reverse;\n",
       "        margin-left: auto\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" \n",
    "      href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css\" \n",
    "      integrity=\"sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2\" \n",
    "      crossorigin=\"anonymous\">\n",
    "<style>\n",
    "    body{margin-top:20px;}\n",
    "\n",
    "    .chat-message-left,\n",
    "    .chat-message-right {\n",
    "        display: flex;\n",
    "        flex-shrink: 0\n",
    "    }\n",
    "\n",
    "    .chat-message-left {\n",
    "        margin-right: auto\n",
    "    }\n",
    "\n",
    "    .chat-message-right {\n",
    "        flex-direction: row-reverse;\n",
    "        margin-left: auto\n",
    "    }\n",
    "</style>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from models/ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 1000\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113748.20 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  = 1000.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
      "gptj_model_load: loading model from 'models/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 4505.45 MB\n",
      "gptj_model_load: memory_size =   896.00 MB, n_mem = 57344\n",
      "gptj_model_load: ..................................."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import GPT4All, LlamaCpp\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llama_embeddings_model = os.environ.get(\"LLAMA_EMBEDDINGS_MODEL\")\n",
    "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
    "\n",
    "model_type = os.environ.get('MODEL_TYPE')\n",
    "model_path = os.environ.get('MODEL_PATH')\n",
    "model_n_ctx = os.environ.get('MODEL_N_CTX')\n",
    "\n",
    "from constants import CHROMA_SETTINGS\n",
    "\n",
    "\n",
    "llama = LlamaCppEmbeddings(model_path=llama_embeddings_model, n_ctx=model_n_ctx)\n",
    "db = Chroma(persist_directory=persist_directory, embedding_function=llama, client_settings=CHROMA_SETTINGS)\n",
    "retriever = db.as_retriever()\n",
    "# Prepare the LLM\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "match model_type:\n",
    "    case \"LlamaCpp\":\n",
    "        llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False)\n",
    "    case \"GPT4All\":\n",
    "        llm = GPT4All(model=model_path, n_ctx=model_n_ctx, backend='gptj', callbacks=callbacks, verbose=False)\n",
    "    case _default:\n",
    "        print(f\"Model {model_type} not supported!\")\n",
    "        exit;\n",
    "\n",
    "# Create the chain and retrieve relevant text chunks, only use the relevant text chunks in the language model\n",
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "EXr9ndwQ-RL3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "\n",
    "def text_eventhandler(*args):\n",
    "    # Needed bc when we \"reset\" the text input\n",
    "    # it fires instantly another event since\n",
    "    # we \"changed\" it's value to \"\"\n",
    "    if args[0][\"new\"] == \"\":\n",
    "        return\n",
    "\n",
    "    # Show loading animation\n",
    "    loading_bar.layout.display = \"block\"\n",
    "\n",
    "    # Get question\n",
    "    question = args[0][\"new\"]\n",
    "\n",
    "    # Reset text field\n",
    "    args[0][\"owner\"].value = \"\"\n",
    "\n",
    "    # Formatting question for output\n",
    "    q = (\n",
    "        f'<div class=\"chat-message-right pb-4\"><div>'\n",
    "        + f'<img src=\"images/bear.png\" class=\"rounded-circle mr-1\" width=\"40\" height=\"40\">'\n",
    "        + f'<div class=\"text-muted small text-nowrap mt-2\">{datetime.now().strftime(\"%H:%M:%S\")}</div></div>'\n",
    "        + '<div class=\"flex-shrink-1 bg-light rounded py-2 px-3 ml-3\">'\n",
    "        + f'<div class=\"font-weight-bold mb-1\">You</div>{question}</div>'\n",
    "    )\n",
    "\n",
    "    # Display formatted question\n",
    "    output.append_display_data(HTML(q))\n",
    "\n",
    "    try:\n",
    "        response = qa({\"question\": f\"{question}\", \"chat_history\": chat_history})\n",
    "        answer = response[\"answer\"]\n",
    "        chat_history.append((question, answer))\n",
    "    except Exception as e:\n",
    "        answer = \"<b>Error:</b> \" + str(e)\n",
    "\n",
    "    # Formatting answer for output\n",
    "    # Replacing all $ otherwise matjax would format them in a strange way\n",
    "    answer_formatted = answer.replace(\"$\", r\"\\$\")\n",
    "    a = (\n",
    "        f'<div class=\"chat-message-left pb-4\"><div>'\n",
    "        + f'<img src=\"images/cat.png\" class=\"rounded-circle mr-1\" width=\"40\" height=\"40\">'\n",
    "        + f'<div class=\"text-muted small text-nowrap mt-2\">{datetime.now().strftime(\"%H:%M:%S\")}</div></div>'\n",
    "        + '<div class=\"flex-shrink-1 bg-light rounded py-2 px-3 ml-3\">'\n",
    "        + f'<div class=\"font-weight-bold mb-1\">LLM</div>{answer_formatted}</div>'\n",
    "    )\n",
    "\n",
    "    # Turn off loading animation\n",
    "    loading_bar.layout.display = \"none\"\n",
    "\n",
    "    output.append_display_data(HTML(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "3okDCxfu-TT_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_text = widgets.Text()\n",
    "in_text.continuous_update = False\n",
    "in_text.observe(text_eventhandler, \"value\")\n",
    "output = widgets.Output()\n",
    "\n",
    "file = open(\"images/loading.gif\", \"rb\")\n",
    "image = file.read()\n",
    "loading_bar = widgets.Image(\n",
    "    value=image, format=\"gif\", width=\"20\", height=\"20\", layout={\"display\": \"None\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Jz9jAh-w-WeV",
    "outputId": "ac58f0ca-4963-4f39-b563-9d5dcadf6da6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e20f43bdf54b8db22c46b43ef501b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(),), layout=Layout(display='inline-flex', flex_flow='column-reverse', max_height='500px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799b5465096e42ba9c8b928e43d25495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Image(value=b'GIF89a\\xc8\\x00\\xc8\\x00\\xf7\\x00\\x00;Ch\\x83\\x90\\xb7\\xcf\\xdc\\xe8\\xda\\xec\\xf1\\xf1\\xf2\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1447.87 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1447.77 ms /     2 tokens (  723.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1449.56 ms\n"
     ]
    }
   ],
   "source": [
    "display(\n",
    "    widgets.HBox(\n",
    "        [output],\n",
    "        layout=widgets.Layout(\n",
    "            width=\"100%\",\n",
    "            max_height=\"500px\",\n",
    "            display=\"inline-flex\",\n",
    "            flex_flow=\"column-reverse\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "display(\n",
    "    widgets.Box(\n",
    "        children=[loading_bar, in_text],\n",
    "        layout=widgets.Layout(display=\"flex\", flex_flow=\"row\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1447.87 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  5009.66 ms /    40 tokens (  125.24 ms per token)\n",
      "llama_print_timings:        eval time =   115.18 ms /     1 runs   (  115.18 ms per run)\n",
      "llama_print_timings:       total time =  5138.63 ms\n"
     ]
    },
    {
     "ename": "NoIndexException",
     "evalue": "Index not found, please create an instance before querying",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNoIndexException\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[76], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m r\u001B[38;5;241m=\u001B[39m \u001B[43mqa\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhat are main risks with 3b5cb84f-ea13-48be-971c-92d76276ab83?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mchat_history\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43mchat_history\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/chains/base.py:140\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    139\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    141\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/chains/base.py:134\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks)\u001B[0m\n\u001B[1;32m    128\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    129\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    130\u001B[0m     inputs,\n\u001B[1;32m    131\u001B[0m )\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    133\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 134\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    137\u001B[0m     )\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    139\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:106\u001B[0m, in \u001B[0;36mBaseConversationalRetrievalChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    105\u001B[0m     new_question \u001B[38;5;241m=\u001B[39m question\n\u001B[0;32m--> 106\u001B[0m docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_docs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_question\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m new_inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    108\u001B[0m new_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m new_question\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:183\u001B[0m, in \u001B[0;36mConversationalRetrievalChain._get_docs\u001B[0;34m(self, question, inputs)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_docs\u001B[39m(\u001B[38;5;28mself\u001B[39m, question: \u001B[38;5;28mstr\u001B[39m, inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[0;32m--> 183\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretriever\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_relevant_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reduce_tokens_below_limit(docs)\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/vectorstores/base.py:366\u001B[0m, in \u001B[0;36mVectorStoreRetriever.get_relevant_documents\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_relevant_documents\u001B[39m(\u001B[38;5;28mself\u001B[39m, query: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 366\u001B[0m         docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvectorstore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity_score_threshold\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    368\u001B[0m         docs_and_similarities \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    369\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search_with_relevance_scores(\n\u001B[1;32m    370\u001B[0m                 query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_kwargs\n\u001B[1;32m    371\u001B[0m             )\n\u001B[1;32m    372\u001B[0m         )\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:181\u001B[0m, in \u001B[0;36mChroma.similarity_search\u001B[0;34m(self, query, k, filter, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity_search\u001B[39m(\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    166\u001B[0m     query: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    170\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[1;32m    171\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m        List[Document]: List of documents most similar to the query text.\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m     docs_and_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_search_with_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [doc \u001B[38;5;28;01mfor\u001B[39;00m doc, _ \u001B[38;5;129;01min\u001B[39;00m docs_and_scores]\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:228\u001B[0m, in \u001B[0;36mChroma.similarity_search_with_score\u001B[0;34m(self, query, k, filter, **kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    227\u001B[0m     query_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embedding_function\u001B[38;5;241m.\u001B[39membed_query(query)\n\u001B[0;32m--> 228\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__query_collection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_results\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mfilter\u001B[39;49m\n\u001B[1;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/utils.py:50\u001B[0m, in \u001B[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     44\u001B[0m     invalid_group_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(arg_groups[i]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m invalid_groups]\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExactly one argument in each of the following\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m groups must be defined:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     48\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(invalid_group_names)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     49\u001B[0m     )\n\u001B[0;32m---> 50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/langchain/vectorstores/chroma.py:120\u001B[0m, in \u001B[0;36mChroma.__query_collection\u001B[0;34m(self, query_texts, query_embeddings, n_results, where, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_results, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_collection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m            \u001B[49m\u001B[43mquery_texts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_texts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m            \u001B[49m\u001B[43mquery_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_results\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    126\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m chromadb\u001B[38;5;241m.\u001B[39merrors\u001B[38;5;241m.\u001B[39mNotEnoughElementsException:\n\u001B[1;32m    128\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m    129\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChroma collection \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_collection\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    130\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontains fewer than \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m elements.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    131\u001B[0m         )\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/chromadb/api/models/Collection.py:219\u001B[0m, in \u001B[0;36mCollection.query\u001B[0;34m(self, query_embeddings, query_texts, n_results, where, where_document, include)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m where_document \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     where_document \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 219\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_results\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_results\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwhere_document\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere_document\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/chromadb/api/local.py:408\u001B[0m, in \u001B[0;36mLocalAPI._query\u001B[0;34m(self, collection_name, query_embeddings, n_results, where, where_document, include)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_query\u001B[39m(\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    401\u001B[0m     collection_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    406\u001B[0m     include: Include \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocuments\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadatas\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistances\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    407\u001B[0m ):\n\u001B[0;32m--> 408\u001B[0m     uuids, distances \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_db\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_nearest_neighbors\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwhere_document\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere_document\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_results\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_results\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m     include_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m include\n\u001B[1;32m    417\u001B[0m     include_documents \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocuments\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m include\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/chromadb/db/clickhouse.py:583\u001B[0m, in \u001B[0;36mClickhouse.get_nearest_neighbors\u001B[0;34m(self, where, where_document, embeddings, n_results, collection_name, collection_uuid)\u001B[0m\n\u001B[1;32m    580\u001B[0m     ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    582\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index(collection_uuid)\n\u001B[0;32m--> 583\u001B[0m uuids, distances \u001B[38;5;241m=\u001B[39m \u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_nearest_neighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_results\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m uuids, distances\n",
      "File \u001B[0;32m~/PycharmProjects/GPT_chroma/venv/lib/python3.11/site-packages/chromadb/db/index/hnswlib.py:230\u001B[0m, in \u001B[0;36mHnswlib.get_nearest_neighbors\u001B[0;34m(self, query, k, ids)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_nearest_neighbors\u001B[39m(\u001B[38;5;28mself\u001B[39m, query, k, ids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 230\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NoIndexException(\n\u001B[1;32m    231\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndex not found, please create an instance before querying\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    232\u001B[0m         )\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;66;03m# Check dimensionality\u001B[39;00m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_dimensionality(query)\n",
      "\u001B[0;31mNoIndexException\u001B[0m: Index not found, please create an instance before querying"
     ]
    }
   ],
   "source": [
    "r= qa({\"question\":\"what are main risks with 3b5cb84f-ea13-48be-971c-92d76276ab83?\",\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mr\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
